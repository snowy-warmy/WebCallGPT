<!DOCTYPE html>
<html lang="nl">
  <head>
    <meta charset="UTF-8" />
    <title>VerduurzaamAdviseur Voice Chat</title>
    <style>
      body {
        font-family: system-ui, sans-serif;
        text-align: center;
        background: #f7f8fa;
        color: #222;
        padding: 3rem;
      }
      h1 {
        color: #007b5f;
      }
      p {
        color: #555;
      }
      button {
        margin-top: 1rem;
        font-size: 1.1rem;
        padding: 0.8rem 1.6rem;
        border: none;
        border-radius: 8px;
        background: #10a37f;
        color: #fff;
        cursor: pointer;
      }
      button:hover {
        background: #0d8a6f;
      }
      #status {
        margin-top: 2rem;
        font-family: monospace;
        color: #333;
        white-space: pre-wrap;
      }
      #transcript {
        margin-top: 1rem;
        background: #fff;
        border-radius: 8px;
        padding: 1rem;
        display: inline-block;
        text-align: left;
        width: 80%;
        max-width: 600px;
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
      }
      #textContent {
        white-space: pre-wrap;
      }
    </style>
  </head>
  <body>
    <h1>üéôÔ∏è VerduurzaamAdviseur</h1>
    <p>Klik op ‚ÄúStart‚Äù en praat ‚Äî de assistent luistert, praat terug en toont de tekst live.</p>
    <button id="start">Start</button>
    <div id="status">Niet verbonden</div>
    <div id="transcript">
      üóíÔ∏è <strong>Live transcript:</strong><br />
      <span id="textContent">...</span>
    </div>

    <script type="module">
      const statusDiv = document.getElementById("status");
      const transcriptDiv = document.getElementById("textContent");
      const startBtn = document.getElementById("start");

      startBtn.onclick = async () => {
        try {
          statusDiv.innerText = "üé§ Vraag microfoon toegang...";
          const mic = await navigator.mediaDevices.getUserMedia({ audio: true });
          statusDiv.innerText = "‚úÖ Microfoon actief. Verbinden met GPT...";

          // Get ephemeral session from backend
          const session = await fetch("/session").then((r) => r.json());
          if (!session?.client_secret?.value) {
            statusDiv.innerText = "‚ùå Fout bij sessie creatie.";
            return;
          }

          const token = session.client_secret.value;
          const model = "gpt-4o-realtime-preview";

          // Setup WebRTC
          const pc = new RTCPeerConnection();
          mic.getTracks().forEach((t) => pc.addTrack(t, mic));

          // üîä GPT audio output
          pc.ontrack = (ev) => {
            const [stream] = ev.streams;
            const audio = document.createElement("audio");
            audio.autoplay = true;
            audio.srcObject = stream;
            document.body.appendChild(audio);
            statusDiv.innerText = "üó£Ô∏è GPT luistert en spreekt terug...";
          };

          // üí¨ Data channel for GPT events
          const dc = pc.createDataChannel("oai-events");
          let liveText = "";
          let userTranscript = "";

          // üß† Handle incoming GPT messages
          dc.onmessage = async (event) => {
            try {
              const msg = JSON.parse(event.data);

              // üëÇ Live transcription (user speech)
              if (msg.type === "input_audio_buffer.transcript.delta") {
                userTranscript += msg.delta;
                transcriptDiv.textContent = userTranscript;
              }

              if (msg.type === "input_audio_buffer.transcript.completed") {
                console.log("üë§ Gebruiker zei:", userTranscript);
                userTranscript += "\n";
              }

              // üí¨ Assistant text
              if (msg.type === "response.text.delta") {
                liveText += msg.delta;
                transcriptDiv.textContent = userTranscript + "\nü§ñ " + liveText;
              }

              if (msg.type === "response.completed") {
                if (liveText.trim().length > 0) {
                  console.log("üí¨ GPT antwoord:", liveText);
                }
                liveText = "";
              }

              // üåê If GPT asks for a Google/Gemini search
              if (msg.action === "search" && msg.query) {
                console.log("üîç GPT vraagt om zoekopdracht:", msg.query);

                // üó£Ô∏è Let the assistant speak naturally before searching
                dc.send(
                  JSON.stringify({
                    type: "response.create",
                    response: {
                      modalities: ["audio", "text"],
                      instructions:
                        "Zeker! Moment alstublieft, ik zoek het even voor u op...",
                    },
                  })
                );

                // üîé Perform Gemini search
                const resp = await fetch(`/search?q=${encodeURIComponent(msg.query)}`);
                const data = await resp.json();

                console.log("üîé Gemini resultaat:", data.result);

                // üé§ Send the results back for a follow-up spoken answer
                dc.send(
                  JSON.stringify({
                    type: "response.create",
                    response: {
                      modalities: ["audio", "text"],
                      instructions: `Ik heb dit gevonden:\n\n${data.result}\n\nGeef nu een kort, vriendelijk antwoord gebaseerd op deze informatie.`,
                    },
                  })
                );
              }

              if (msg.type === "error") {
                console.error("‚ö†Ô∏è OpenAI error:", msg.error);
              }
            } catch (err) {
              // ignore binary packets
            }
          };

          // üîó When ready
          dc.onopen = () => {
            console.log("‚úÖ DataChannel open");
            statusDiv.innerText = "Verbonden! GPT zal zo iets zeggen...";
            dc.send(
              JSON.stringify({
                type: "session.update",
                session: {
                  modalities: ["audio", "text"],
                  voice: "coral",
                  instructions:
                    "Je bent de VerduurzaamAdviseur. Geef korte, vriendelijke, praktische antwoorden in het Nederlands (max. 3 zinnen).",
                },
              })
            );

            // Start conversation
            dc.send(
              JSON.stringify({
                type: "response.create",
                response: {
                  modalities: ["audio", "text"],
                  instructions:
                    "Begroet me kort en vriendelijk alsof we net beginnen te praten.",
                },
              })
            );
          };

          // üîÑ SDP handshake
          const offer = await pc.createOffer();
          await pc.setLocalDescription(offer);

          const resp = await fetch(
            `https://api.openai.com/v1/realtime?model=${model}`,
            {
              method: "POST",
              headers: {
                Authorization: `Bearer ${token}`,
                "Content-Type": "application/sdp",
                "OpenAI-Beta": "realtime=v1",
              },
              body: offer.sdp,
            }
          );

          if (!resp.ok) {
            statusDiv.innerText = "‚ùå Verbinding mislukt.";
            console.error(await resp.text());
            return;
          }

          const answer = { type: "answer", sdp: await resp.text() };
          await pc.setRemoteDescription(answer);
          statusDiv.innerText =
            "‚úÖ Verbonden ‚Äî praat nu met de VerduurzaamAdviseur.";
        } catch (err) {
          console.error("‚ùå Setup error:", err);
          statusDiv.innerText = "‚ùå Fout bij verbinden of microfoon.";
        }
      };
    </script>
  </body>
</html>
