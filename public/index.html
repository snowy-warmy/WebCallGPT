<!DOCTYPE html>
<html lang="nl">
<head>
  <meta charset="UTF-8" />
  <title>Verduurzamen Voice Chat</title>
  <style>
    body { font-family: sans-serif; background: #f6f8fa; text-align: center; padding: 2em; }
    h1 { color: #007b5f; }
    button { margin: 1em; padding: 0.7em 1.2em; font-size: 1.1em; border: none; border-radius: .4em; cursor: pointer; }
    #start { background-color: #007b5f; color: #fff; }
    #stop { background-color: #b33; color: #fff; }
    #status { margin-top: 1em; font-weight: bold; color: #444; }
  </style>
</head>
<body>
  <h1>🎙️ Verduurzamen Voice Assistant</h1>
  <p>Druk op “Start Talking”, spreek Nederlands en de assistent praat terug!</p>
  <button id="start">Start Talking</button>
  <button id="stop">Stop</button>
  <div id="status">Status: <span id="indicator">Idle</span></div>
  <audio id="assistantAudio" autoplay></audio>

  <script>
    let ws, mediaRecorder, audioContext;
    const indicator = document.getElementById("indicator");

    function setStatus(text, color="#444") {
      indicator.textContent = text;
      indicator.style.color = color;
    }

    document.getElementById("start").onclick = async () => {
      setStatus("Connecting...", "#888");
      ws = new WebSocket(`wss://${window.location.host}`);
      ws.binaryType = "arraybuffer";

      ws.onopen = () => {
        console.log("🔗 Connected to backend");
        setStatus("Listening...", "#007b5f");
      };

      ws.onmessage = async (event) => {
        try {
          const msg = JSON.parse(event.data);

          // 🎧 Audio playback
          if (msg.type === "response.audio.delta") {
            console.log("🎧 Audio delta received:", msg.delta.length);
            const audioBytes = Uint8Array.from(atob(msg.delta), c => c.charCodeAt(0));
            if (!audioContext) audioContext = new AudioContext();
            const audioBuffer = await audioContext.decodeAudioData(audioBytes.buffer);
            const src = audioContext.createBufferSource();
            src.buffer = audioBuffer;
            src.connect(audioContext.destination);
            src.start();
          }

          // 💬 Text transcript
          if (msg.type === "response.text.delta") {
            console.log("🤖 GPT:", msg.delta);
          }

          if (msg.type === "error") {
            console.error("⚠️ OpenAI error:", msg.error);
          }
        } catch {
          // ignore binary chunks
        }
      };

      // 🎙️ Capture microphone
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream, { mimeType: "audio/webm;codecs=opus" });

      mediaRecorder.ondataavailable = async (e) => {
        setStatus("Sending audio...", "#ff9500");
        const reader = new FileReader();
        reader.onloadend = () => {
          const base64Audio = reader.result.split(",")[1];

          // ✅ Tell OpenAI the audio format is webm
          ws.send(JSON.stringify({ 
            type: "input_audio_buffer.append", 
            audio: base64Audio,
            format: "webm" 
          }));
          ws.send(JSON.stringify({ type: "input_audio_buffer.commit" }));
          ws.send(JSON.stringify({
            type: "response.create",
            response: {
              modalities: ["audio", "text"],
              instructions: "Beantwoord vriendelijk in het Nederlands en zeg iets over verduurzaming."
            }
          }));
          setStatus("Listening...", "#007b5f");
        };
        reader.readAsDataURL(e.data);
      };

      mediaRecorder.start(1500); // send every 1.5s
    };

    document.getElementById("stop").onclick = () => {
      setStatus("Stopped", "#b33");
      mediaRecorder?.stop();
      ws?.close();
    };
  </script>
</body>
</html>
